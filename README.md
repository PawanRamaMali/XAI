# üß† Explainable AI (XAI) for Image Classification

This repository provides implementations of four **Explainable AI (XAI) methods** for interpreting deep learning models in image classification. These methods generate **heatmaps** that highlight the most important regions of an image contributing to the model's prediction.

## üöÄ Methods Implemented
1. **Class Activation Mapping (CAM)**
2. **Grad-CAM (Gradient-weighted Class Activation Mapping)**
3. **Grad-CAM++**
4. **Score-CAM (Score-weighted CAM)**

Each of these methods helps visualize how a deep learning model (e.g., **ResNet50**) makes decisions, improving interpretability for AI practitioners, researchers, and domain experts.


## üìå 1. Class Activation Mapping (CAM)
**üìñ Description:**  

- **CAM** is an early explainability method that highlights important image regions based on **global average pooling (GAP)** applied to convolutional feature maps.

- It requires **modifying the CNN architecture** to replace fully connected (FC) layers with a **GAP layer**, making it applicable **only to specific network architectures**.

**üìä How It Works:**

1. Uses **feature maps** from the last convolutional layer.
2. Applies **GAP** to obtain class-specific weights.
3. Computes a weighted sum of the activation maps.

**‚ö†Ô∏è Limitations:**

- Requires architectural modification, making it incompatible with pre-trained models.
- Less accurate than later methods like **Grad-CAM**.

---

## üìå 2. Grad-CAM (Gradient-weighted CAM)
**üìñ Description:**  

- **Grad-CAM** improves CAM by using **gradients of the target class** to **weigh the activation maps**, highlighting key regions **without modifying the network architecture**.
- It is **compatible with any CNN-based model**.

**üìä How It Works:**

1. Captures **feature maps** from the last convolutional layer.
2. Computes **gradients** of the target class score **w.r.t.** these feature maps.
3. Performs a **weighted sum** of the feature maps using the computed gradients.
4. Applies **ReLU activation** to focus only on important regions.

**‚úÖ Advantages:**

- **Works with any CNN model** without modifications.
- Provides **class-specific heatmaps**, useful for interpreting model decisions.

**‚ö†Ô∏è Limitations:**

- Can miss **fine-grained** details in complex images.
- Focuses only on **positive influences**, ignoring negative contributions.

---

## üìå 3. Grad-CAM++ (Enhanced Grad-CAM)
**üìñ Description:**  

- **Grad-CAM++** is an improvement over Grad-CAM that assigns **better weight distributions** to activation maps, improving localization.
- It captures **multiple important regions**, making it more precise for **overlapping objects**.

**üìä How It Works:**

1. Computes **first-order and second-order gradients**.
2. Uses these gradients to **refine the weighting** of activation maps.
3. Generates **more localized and precise heatmaps**.

**‚úÖ Advantages:**

- **Better localization** than Grad-CAM.
- Captures **multiple regions** of interest instead of just one.
- More robust for **complex images**.

**‚ö†Ô∏è Limitations:**

- **Higher computational cost** than Grad-CAM.
- Requires computing **higher-order gradients**.

---

## üìå 4. Score-CAM (Score-weighted CAM)
**üìñ Description:**  

- **Score-CAM** removes the need for **gradients**, making it a **gradient-free** interpretability method.
- It **perturbs the input image** using feature maps and measures the change in the model's confidence score.

**üìä How It Works:**

1. Extracts **activation maps** from the last convolutional layer.
2. **Perturbs** the original image by multiplying it with each activation map.
3. Computes the **model's confidence score** for each perturbed image.
4. Uses these confidence scores to **weight the activation maps**.

**‚úÖ Advantages:**

- **Does not require gradients**, making it model-agnostic.
- Produces **better heatmap localization** than Grad-CAM.
- Works well with **any CNN model**.

**‚ö†Ô∏è Limitations:**

- **Computationally expensive**, as it requires multiple forward passes.
- Sensitive to **perturbation variations**.

---

## üìå Comparison of XAI Methods

| Method     | Requires Gradients? | Model Modification? | Computational Cost | Localization Quality |
|------------|-------------------|-------------------|------------------|---------------------|
| **CAM**       | ‚ùå No  | ‚úÖ Yes  | ‚≠ê Fast  | ‚ö†Ô∏è Low |
| **Grad-CAM**  | ‚úÖ Yes | ‚ùå No   | ‚≠ê‚≠ê Medium | ‚≠ê‚≠ê Good |
| **Grad-CAM++**| ‚úÖ Yes | ‚ùå No   | ‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê‚≠ê Better |
| **Score-CAM** | ‚ùå No  | ‚ùå No   | ‚≠ê‚≠ê‚≠ê‚≠ê Very High | ‚≠ê‚≠ê‚≠ê‚≠ê Best |

---

## üõ† How to Run

### **üîπ Setup**
1. Install dependencies:
   ```bash
   pip install torch torchvision numpy opencv-python matplotlib
   ```

2. Clone the repository:
   ```bash
   git clone https://github.com/PawanRamaMali/XAI.git
   cd XAI
   ```

3. Run any of the methods:
   ```bash
   python grad_cam.py  # For Grad-CAM
   python grad_cam_plus.py  # For Grad-CAM++
   python score_cam.py  # For Score-CAM
   ```
---


## üéØ **Use Cases**
‚úÖ **Medical Imaging:** Identifying critical regions in **X-rays, MRIs, and CT scans**.  
‚úÖ **Autonomous Vehicles:** Understanding how a model detects **traffic signs, pedestrians, and obstacles**.  
‚úÖ **Security & Forensics:** Interpreting **face recognition models and fraud detection systems**.  
‚úÖ **AI Fairness & Bias Detection:** Ensuring models focus on **relevant, unbiased features**.  

---


### **üîπ Summary of Explainability Methods in XAI**  

This table provides a high-level **comparison of different XAI methods**, including their **type, key approach, best use case, and pros & cons.**  

| **Method**           | **Type**       | **Approach** | **Best For** | **Pros ‚úÖ** | **Cons ‚ùå** |
|----------------------|---------------|-------------|-------------|------------|------------|
| **CAM (Class Activation Mapping)** | Local | Feature map weighting | **CNNs** | Fast, simple | Requires network modification |
| **Grad-CAM** | Local | Gradients | **CNNs, Image Classification** | Works on pre-trained models | Can be coarse |
| **Grad-CAM++** | Local | Improved Gradients | **CNNs, Multiple Objects** | Better localization | More computationally expensive |
| **Score-CAM** | Local | Perturbation-based | **CNNs (No gradients needed)** | Model-agnostic | Computationally expensive |
| **LIME** | Local | Perturbation & Surrogate Model | **Any Model, NLP, Tabular, Images** | Model-agnostic | Can be unstable, only local |
| **SHAP (Shapley Values)** | Local & Global | Game Theory | **Any Model, Global Interpretability** | Fair, consistent feature attribution | Computationally expensive |
| **Kernel SHAP** | Local & Global | Approximate SHAP | **Large Datasets, ML Models** | Faster than SHAP | Less accurate |
| **Integrated Gradients** | Local | Gradient-based | **Deep Learning, NLP** | Captures non-linearity | Requires differentiability |
| **DeepLIFT** | Local | Reference-based Activation Difference | **Deep Learning, Medical AI** | Faster than IG | Requires reference selection |
| **Permutation Importance** | Global | Feature Shuffling | **Feature Selection, ML Models** | Simple, fast | Doesn't capture feature interactions |
| **Occlusion Sensitivity** | Local | Masking & Perturbation | **CNNs, Medical Imaging** | Works on any CNN | Computationally expensive |
| **Contrastive Explanations Method (CEM)** | Local | Contrastive Learning | **Bias Detection, NLP** | Finds minimal feature changes | Computationally expensive |
| **XGBoost Feature Importance** | Global | Gain-based ranking | **Tree-Based Models** | Built-in, fast | Doesn‚Äôt explain interactions |

---

## **üîπ Which Method Should You Use?**
### **1Ô∏è‚É£ Explaining Deep Learning Models (CNNs & NLP)**
| Use Case | Best Methods |
|----------|-------------|
| **Explaining CNN decisions (Image AI)** | **Grad-CAM, Score-CAM, Occlusion Sensitivity** |
| **Explaining multiple objects in an image** | **Grad-CAM++, Score-CAM** |
| **Explaining NLP models** | **Integrated Gradients, DeepLIFT, SHAP** |

### **2Ô∏è‚É£ Explaining Any Machine Learning Model**
| Use Case | Best Methods |
|----------|-------------|
| **Feature Importance for Any Model** | **SHAP, LIME, Permutation Importance** |
| **Ensuring fairness & bias detection** | **SHAP, CEM, Permutation Importance** |
| **Explaining ensemble models (XGBoost, Random Forests)** | **SHAP, XGBoost Feature Importance** |

### **3Ô∏è‚É£ Fast Interpretability & Feature Selection**
| Use Case | Best Methods |
|----------|-------------|
| **Quick interpretation of black-box models** | **LIME, SHAP, Kernel SHAP** |
| **Feature selection for better performance** | **Permutation Importance, SHAP** |

---

## **üîπ Key Takeaways**
1. **Grad-CAM & Score-CAM** ‚Üí Best for **CNNs (image models)**.  
2. **SHAP** ‚Üí Best for **global & local explanations (fair and consistent)**.  
3. **LIME** ‚Üí Good for **fast, local interpretations** but less stable.  
4. **Integrated Gradients & DeepLIFT** ‚Üí Best for **deep learning models (NLP, medical AI)**.  
5. **Permutation Importance** ‚Üí Simple but effective for **feature selection**.  

---

## üìú License
This project is licensed under the **MIT License**.

---

## üì¨ Contact
For questions or collaborations, feel free to reach out:
- üìß Email: prm@outlook.in

